{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Personality Detection Using Emotion Frequencies\n",
    "*Written by Mark Endo*\n",
    "## Introduction: Capturing an Author's Expression\n",
    "In class, we determined the author of Federalist Paper 49 using the \"Bag of Words\" technique which compared word probabilities for Madison and Hamilton. This approach produced promising results, but it only looked at one aspect of writing. In reality, people uniquely express themselves (and therefore identify themselves) through much more than the frequency of words that they use. Writers have personalities that come through their writing, which cannot solely be captured by a count of individual words. I'm interested in finding other ways to identify the *person* behind the text. Particularly, **can we determine the author of a document based solely off of the writing's emotional expression?**\n",
    "\n",
    "The concept of decoding emotional expression from text has countless applications. Here are a few that seem particularly interesting and impactful:\n",
    " - Solving disputes about who wrote a particular part of a body of work (focus of this project).\n",
    " - Advanced plagiarism detection that goes beyond word/sentence structure and examines tones of writing.\n",
    " - Writing tools to help users understand their tone, like Grammarly's new [tone detector.](https://www.grammarly.com/blog/tone-detector/)\n",
    " - Speech analytics tools to help speakers strengthen their \"voice.\"\n",
    "\n",
    "In this project, I tackle the first problem of detecting authors given training and test data, but the underlying concepts of the work can be applied to the other applications. With a refined model of emotional extraction, the possibilities are endless.\n",
    "## How To Capture Expression?\n",
    "\n",
    "The first step in solving any of these problems is finding some way of capturing a text's emotional expression. For this, we cannot simply use the \"Bag of Words\" model. The tone of a piece relies on more than the frequencies of words that are used. It also depends on the ordering of the words, and how the words relate to each other. For example, let's look at an example sentence, \n",
    "\n",
    "> \"The pizza is looking awfully tasty.\"\n",
    "\n",
    " With our understanding of language, we know that the tone of the sentence is positive, as the phrase \"awfully tasty\" means very tasty. However, if we were to ignore the relation between words, we could think the tone was neutral to negative as one definition of awfully means very bad. So, we must have a method that captures the *context* of the words. To do this, we will employ an NLP technique of [context analysis](https://www.lexalytics.com/lexablog/context-analysis-nlp). Using context analysis, we can break the text up into words and phrases that have meaning. For example, the phrase \"awfully tasty\" in the previous example is a context that has textual meaning.\n",
    "\n",
    "So now that we have a way to capture the relation between words, how can we figure out what emotions the contexts represent? To do this, we will use a database provided by [SenticNet](https://sentic.net/) that maps contexts to emotional scores. The scores represent the positivity (or negativity) of a context. If the number is positive, then it is positive. Otherwise, it is negative. With these context scores, we will use the frequencies of scores to determine the probability that someone exhibits the same emotional scores for the entire text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Creating a dictionary of all contexts from SenticNet\n",
    "\n",
    "First, we create a dictionary that maps all contexts from SenticNet to their corresponding emotional scores. We do this so we can later easily access them when extracting a text's contexts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "contexts_filename = 'senticnet5.txt'\n",
    "contexts_dict = {}\n",
    "\n",
    "with open(contexts_filename) as f:\n",
    "    for line in f:\n",
    "        info = line.split()\n",
    "        contexts_dict[info[0]] = info[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Extracting contexts from a file\n",
    "\n",
    "In order to analyze the tone of documents, we first have to extract all of the contexts from the documents in question. To do so, we will use n-grams. According to Lexalytics,\n",
    "> N-grams are combinations of one or more words that represent entities, phrases, concepts, and themes that appear in text.\n",
    "\n",
    "One benefit of using n-grams is that it does not rely on using deep learning libraries, and is therefore simple to implement and use. The main downside of this method is that it only captures phrases that are contiguous. In other words, it cannot capture contexts of words that are split up. For example, if we are working with the phrase \"sad, old man\", sad man will not be captured as a context. Typically, n-grams also include a lot of \"noise,\" as a lot of phrases are words with little meaning such as \"with that.\" However, our implementation is able to reduce this noise, since we only include contexts that are in the SenticNet library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getContextsFromFile(filename):\n",
    "    text_list = []\n",
    "    with open(filename) as f:\n",
    "        text_list = f.read().split()\n",
    "    preprocessed_text_list = list(map(lambda word: word.lower().strip('!:;,'), text_list))\n",
    "    contexts = []\n",
    "    for i in range(len(preprocessed_text_list)):\n",
    "        num_words_considering = 1\n",
    "        while(True):\n",
    "            if i + num_words_considering > len(preprocessed_text_list): #break out if you are going past the end of the text\n",
    "                break\n",
    "            potential_context = ''\n",
    "            for j in range(num_words_considering):\n",
    "                if j == 0:\n",
    "                    potential_context += preprocessed_text_list[i + j]\n",
    "                else:\n",
    "                    potential_context += '_' + preprocessed_text_list[i + j]\n",
    "            if potential_context in contexts_dict:\n",
    "                contexts.append((potential_context, float(contexts_dict[potential_context])))\n",
    "                num_words_considering += 1\n",
    "            else:\n",
    "                break\n",
    "    return contexts\n",
    "\n",
    "madisonContexts = getContextsFromFile('madison.txt')\n",
    "hamiltonContexts = getContextsFromFile('hamilton.txt')\n",
    "unknownContexts = getContextsFromFile('unknown.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aside: Getting Overall Tone Score\n",
    "One thing we can do with our contexts is find an overall tone score for a document. This is done by totaling all of the scores for each context in the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Madison overall mood: 630.9989999999962\n",
      "Hamilton overall mood: 536.512\n",
      "Unknown doc. overall mood: 487.19800000000083\n"
     ]
    }
   ],
   "source": [
    "def findOverallMood(contexts):\n",
    "    total = 0\n",
    "    for context, score in contexts:\n",
    "        total += score\n",
    "    return total\n",
    "\n",
    "\n",
    "madison_score = findOverallMood(madisonContexts)\n",
    "hamilton_score = findOverallMood(hamiltonContexts)\n",
    "unknown_score = findOverallMood(unknownContexts)\n",
    "\n",
    "print('Madison overall mood: ' + str(madison_score))\n",
    "print('Hamilton overall mood: ' + str(hamilton_score))\n",
    "print('Unknown doc. overall mood: ' + str(unknown_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here, we may be tempted to use our results as a form of author detection. However, I strongly warn against this. The overall tone score only captures the general sentiment (positive or negative) of the document as a whole. This takes away all nuances of particular emotional expression. For example, a writer that uses both extremely positive and negative language can get the same score as a writer that uses neutral language throughout. In order to find the more likely author, we will use the probabilities of each writer expressing exact emotions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Generating emotion probability lookups from known writings\n",
    "Instead of looking at an overall emotion score, we will look at the count of how many times each emotion appears. Here, we are defining an emotion as the score corresponding to a context from the text. From here, we can create a probability lookup `emotionProbMap` that stores $P(emotion|writer)$. Then, we can implement `getEmotionProb(emotionProbMap, emotion)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(.12|madison) =  0.0006257822277847309\n",
      "P(.12|hamilton) =  0.0007479431563201197\n"
     ]
    }
   ],
   "source": [
    "EPSILON = 0.000001\n",
    "\n",
    "def makeEmotionProbMap(contexts):\n",
    "    emotionProbMap = {}\n",
    "    num_contexts = 0\n",
    "    for context, emotion in contexts:\n",
    "        if emotion not in emotionProbMap:\n",
    "            emotionProbMap[emotion] = 0\n",
    "        emotionProbMap[emotion] += 1\n",
    "        num_contexts += 1\n",
    "    for emotion in emotionProbMap:\n",
    "        emotionProbMap[emotion] /= num_contexts\n",
    "    return emotionProbMap\n",
    "        \n",
    "madisonEmotionProb = makeEmotionProbMap(madisonContexts)\n",
    "hamiltonEmotionProb = makeEmotionProbMap(hamiltonContexts)\n",
    "    \n",
    "def getEmotionProb(emotionProbMap, emotion):\n",
    "    if emotion in emotionProbMap:\n",
    "        return emotionProbMap[emotion]\n",
    "    return EPSILON\n",
    "\n",
    "print(\"P(.12|madison) = \", getEmotionProb(madisonEmotionProb, .12))\n",
    "print(\"P(.12|hamilton) = \", getEmotionProb(hamiltonEmotionProb, .12))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Generating the emotion counts from the unknown document\n",
    "\n",
    "Here, we map the emotions (context scores) to the number of times each appears in the unknown text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# unique emotions in unknown.txt: 239\n",
      "# of times .12 appears in unknown.txt: 3\n"
     ]
    }
   ],
   "source": [
    "def makeEmotionCountMap(contexts):\n",
    "    emotionCountMap = {}\n",
    "    for context, emotion in contexts:\n",
    "        if emotion not in emotionCountMap:\n",
    "            emotionCountMap[emotion] = 0\n",
    "        emotionCountMap[emotion] += 1\n",
    "    return emotionCountMap\n",
    "\n",
    "unknownDocCount = makeEmotionCountMap(unknownContexts)\n",
    "print('# unique emotions in unknown.txt:', len(unknownDocCount))\n",
    "print('# of times .12 appears in unknown.txt:', unknownDocCount[.12])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Using Bayes' Theorem and Computing Log Probabilities\n",
    "\n",
    "From Bayes' Theorem, we can say:\n",
    "\n",
    "$$P(writer|unknownDoc) = \\frac{P(unknownDoc|writer)P(writer)}{P(unknownDoc)}$$\n",
    "\n",
    "Since we are working with a ratio of two probabilities, we only worry about the numerator.\n",
    "\n",
    "We will model the distribution of emotion counts in an unknown document (conditioned on knowing the writer) as a Multinomial RV. We can compute a ratio of the product of probabilities of observing each emotion given each author wrote it.\n",
    "\n",
    "$$P(unknownDoc|Madison) \\propto \\prod_{i=1}^{m} (p_{M, i}^{\\text{# appearances of emotion i in unknown}})$$\n",
    "\n",
    "Since we are working with very small probabilities, in order to avoid overflow, we will calculate log probabilities as so:\n",
    "\n",
    "$logP(unknownDoc|Madison)−logP(unknownDoc|Hamilton)>0 → \\text{Madison wrote document}$,\n",
    "\n",
    "where $P(unknownDoc|Madison) \\propto \\sum_{i=1}^{m}(\\text{(# appearances of emotion i in unknown)}log(p_{M, i}))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log madison: \t\t -5960.401251604811\n",
      "log hamilton: \t\t -6171.8374006506065\n",
      "log madison/hamilton:\t 211.4361490457959\n"
     ]
    }
   ],
   "source": [
    "def calcLogProbDoc(emotionProbMap, countMap):\n",
    "    logprob = 0\n",
    "    for emotion in countMap:\n",
    "        c_i = countMap[emotion]\n",
    "        p_i = getEmotionProb(emotionProbMap, emotion)\n",
    "        logprob += c_i * math.log(p_i)\n",
    "    return logprob\n",
    "\n",
    "\n",
    "logpMadison = calcLogProbDoc(madisonEmotionProb, unknownDocCount)\n",
    "logpHamilton = calcLogProbDoc(hamiltonEmotionProb, unknownDocCount)\n",
    "print('log madison: \\t\\t',logpMadison)\n",
    "print('log hamilton: \\t\\t', logpHamilton)\n",
    "print('log madison/hamilton:\\t',logpMadison - logpHamilton)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future Research\n",
    "There are many areas in which this research can be further explored. First, we could consider more than the positivity/negativity of contexts. To do so, each context score could store a range of emotions, and the score would be considered \"the same\" if the individual emotions are within some range of each other. Second, we could weight the different contexts, since some contexts seem to be more critical than others. For example, we could give more emphasis on the emotions of 2-grams compared to 1-grams, as 2-grams generally hold more meaning. These are just a few suggestions, but there are many other areas of research/applications that can be explored."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
